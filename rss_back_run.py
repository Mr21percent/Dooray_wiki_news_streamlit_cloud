import pandas as pd
from datetime import datetime, timedelta, time
from zoneinfo import ZoneInfo
from holidayskr import is_holiday
import feedparser
import requests
from collections import defaultdict
from bs4 import BeautifulSoup
import html
import json
import os
import glob
import streamlit as st
from dooray_api_client import DoorayAPIClient
from apscheduler.schedulers.background import BackgroundScheduler
from apscheduler.triggers.cron import CronTrigger
import time as time_module

# RSS URL ë”•ì…”ë„ˆë¦¬
rss_url_dict = {
    'ê¸ˆìœµìœ„ì›íšŒ':'https://www.korea.kr/rss/dept_fsc.xml',
    'ê¸°íšì¬ì •ë¶€': 'https://www.korea.kr/rss/dept_moef.xml',
    'ì‚°ì—…í†µìƒìì›ë¶€' : 'https://www.korea.kr/rss/dept_motie.xml',
    'ê³¼í•™ê¸°ìˆ ì •ë³´í†µì‹ ë¶€' : 'https://www.korea.kr/rss/dept_msit.xml',
    'ì¤‘ì†Œë²¤ì²˜ê¸°ì—…ë¶€' : 'https://www.korea.kr/rss/dept_mss.xml',
    'íƒ„ì†Œì¤‘ë¦½ë…¹ìƒ‰ì„±ì¥ ìœ„ì›íšŒ' : 'https://www.korea.kr/rss/dept_cnc.xml',
}

# JSON ì„¤ì • íŒŒì¼ ë¡œë“œ í•¨ìˆ˜
def load_settings(folder="task_list"):
    """
    task_list í´ë”ì—ì„œ ëª¨ë“  JSON ì„¤ì • íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤.
    """
    settings = []
    try:
        if not os.path.exists(folder):
            os.makedirs(folder)
            return settings
            
        json_files = glob.glob(os.path.join(folder, "*.json"))
        for file_path in json_files:
            with open(file_path, 'r', encoding='utf-8') as f:
                setting = json.load(f)
                setting_name = os.path.basename(file_path).replace("_data.json", "")
                setting["setting_name"] = setting_name
                settings.append(setting)
        return settings
    except Exception as e:
        print(f"ì„¤ì • íŒŒì¼ ë¡œë”© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return []

def get_start_date_and_time():
    """
    ì‹œì‘ ë‚ ì§œì™€ ì‹œê°„ ê³„ì‚° í•¨ìˆ˜
    """
    # 'Asia/Seoul' íƒ€ì„ì¡´ ì •ë³´ ì‚¬ìš©
    kst = ZoneInfo("Asia/Seoul")
    korea_time = datetime.now(kst)
    cur_date = korea_time.strftime("%Y-%m-%d")
    
    start_time = korea_time - timedelta(days=1)
    
    while is_holiday(start_time.strftime("%Y-%m-%d")):
        start_time = start_time - timedelta(days=1)
    
    start_date = start_time.strftime("%Y-%m-%d")
    start_date_6pm = datetime.combine(start_time, time(17, 30), tzinfo=kst)
    
    return start_date, start_date_6pm, cur_date, start_time

def clean_summary(summary_html):
    """
    HTML ìš”ì•½ ì •ë³´ë¥¼ ì •ë¦¬í•˜ëŠ” í•¨ìˆ˜
    """
    try:
        soup = BeautifulSoup(summary_html, 'html5lib')

        # ì´ë¯¸ì§€ë‚˜ ë§í¬ ë“± ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°
        for tag in soup(['a', 'img', 'figure', 'figcaption', 'div']):
            tag.decompose()

        # í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ + HTML entity ë³µí˜¸í™”
        clean_text = html.unescape(soup.get_text(separator=' ', strip=True))

        # ê³µë°± ì •ë¦¬
        clean_text = ' '.join(clean_text.split())

        return clean_text
    except Exception as e:
        print(f"ìš”ì•½ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return summary_html

def fetch_rss_data(rss_url_dict):
    """
    RSS í”¼ë“œì—ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
    """
    all_entries = []

    # RSS í”¼ë“œë¥¼ URL ë”•ì…”ë„ˆë¦¬ì—ì„œ ë°›ì•„ ì²˜ë¦¬
    for dept_name, rss_url in rss_url_dict.items():
        try:
            feed = feedparser.parse(rss_url)
            for entry in feed.entries:
                all_entries.append({
                    'department': dept_name,
                    'title': entry.title,
                    'link': entry.link,
                    'published': entry.published,
                    'summary': entry.summary if hasattr(entry, 'summary') else ''
                })
        except Exception as e:
            print(f"{dept_name} RSS ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    # DataFrame ìƒì„±
    if not all_entries:
        return pd.DataFrame()
        
    rss_df = pd.DataFrame(all_entries)
    rss_df['summary'] = rss_df['summary'].apply(clean_summary)

    # í•œêµ­ ì‹œê°„ëŒ€ë¡œ ë³€í™˜
    kst = ZoneInfo("Asia/Seoul")
    rss_df['published'] = pd.to_datetime(rss_df['published'], utc=True).dt.tz_convert(kst)

    return rss_df


def generate_markdown(df, start_time, korea_time, use_gpt=False, gpt_prompt=""):
    """
    ìˆ˜ì§‘í•œ ë°ì´í„°ë¥¼ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜
    """
    markdown_output = f"# {start_time.strftime('%y%m%d')}~{korea_time.strftime('%y%m%d')} ë³´ë„ìë£Œ\n\n"

    grouped = defaultdict(list)
    for _, row in df.iterrows():
        grouped[row['department']].append(row)

    for dept, items in grouped.items():
        markdown_output += f"## {dept}\n"
        for row in items:
            pub_time = pd.to_datetime(row['published']).strftime('%Y-%m-%d %Hì‹œ')
            markdown_output += f"- **{row['title']}** [[ë§í¬]]({row['link']})  \n"
            markdown_output += f"  <sub>({pub_time})</sub>\n\n"
            
            summary = row['summary']
            if use_gpt and summary and gpt_prompt:
                #summary = apply_gpt_summary(summary, gpt_prompt)
                pass
            if summary:
                markdown_output += f"  {summary}\n\n"
            else:
                markdown_output += f"  ìš”ì•½ ì •ë³´ ì—†ìŒ\n\n"
                
    return markdown_output

# ... (ìƒëµ: ê¸°ì¡´ import ë° í•¨ìˆ˜ ì •ì˜ ë¶€ë¶„ ë™ì¼)

def fetch_and_upload_news(setting=None, rss_df=None, progress_bar=None, status=None):
    """
    ì´ë¯¸ ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ë°›ì•„ Dooray Wikiì— ì—…ë¡œë“œí•˜ëŠ” ë©”ì¸ í•¨ìˆ˜
    """
    try:
        def update_status(step_num, total_steps, message):
            if progress_bar is not None:
                progress_bar.progress(step_num / total_steps)
            if status is not None:
                status.update(label=f"Step {step_num}/{total_steps}: {message}")

        total_steps = 5
        cur_steps = 1
        
        update_status(cur_steps, total_steps, "ë‚ ì§œ ê³„ì‚° ì¤‘...")
        cur_steps += 1
        start_date, start_date_6pm, cur_date, start_time_obj = get_start_date_and_time()

        if rss_df is None or rss_df.empty:
            return False, "RSS ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."
        
        update_status(cur_steps, total_steps, "ë‰´ìŠ¤ í•„í„°ë§ ì¤‘...")
        cur_steps += 1
        today_full_news_df = rss_df.query("published >= @start_date_6pm")
        
        if today_full_news_df.empty:
            return False, "í•„í„°ë§ í›„ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤."

        update_status(cur_steps, total_steps, "ë§ˆí¬ë‹¤ìš´ ìƒì„± ì¤‘...")
        cur_steps += 1
        markdown_output = generate_markdown(
            today_full_news_df,
            start_time_obj,
            datetime.now(ZoneInfo("Asia/Seoul")),
            False,
            ""
        )

        if setting and setting.get("wiki_id") and setting.get("page_id"):
            update_status(cur_steps, total_steps, "Dooray Wikiì— ì—…ë¡œë“œ ì¤‘...")
            cur_steps += 1
            user_name = setting.get("user_name")
            dooray_token = None

            if hasattr(st, "secrets"):
                for user_info in st.secrets.values():
                    if isinstance(user_info, dict) and user_info.get("name") == user_name:
                        dooray_token = user_info.get("Dooray_token")
                        break

            if not dooray_token:
                return False, f"'{user_name}' ì‚¬ìš©ìì˜ Dooray í† í°ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

            client = DoorayAPIClient(token=dooray_token)
            success, page_id = client.create_wiki_page(
                setting["wiki_id"],
                setting["page_id"],
                f"ë‰´ìŠ¤ ì—…ë°ì´íŠ¸ {cur_date}",
                markdown_output
            )

            update_status(total_steps, total_steps, "ì™„ë£Œ!")
            return (True, "ì—…ë¡œë“œ ì„±ê³µ") if success else (False, "Dooray ì—…ë¡œë“œ ì‹¤íŒ¨")
        else:
            update_status(total_steps, total_steps, "ì™„ë£Œ!")
            return True, markdown_output

    except Exception as e:
        return False, f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}"


def job():
    print("\nğŸ•’ ì‘ì—… ì‹œì‘:", datetime.now().strftime('%Y-%m-%d %H:%M:%S'))
    rss_df = fetch_rss_data(rss_url_dict)

    if rss_df.empty:
        print("âŒ RSS ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
        return

    for setting in load_settings():
        print(f"ğŸ“„ ì„¤ì • ì²˜ë¦¬ ì¤‘: {setting.get('setting_name', 'ì´ë¦„ ì—†ìŒ')}")
        success, result = fetch_and_upload_news(setting, rss_df=rss_df)
        if success:
            print(f"âœ… ì—…ë¡œë“œ ì„±ê³µ: {result}")
        else:
            print(f"âŒ ì—…ë¡œë“œ ì‹¤íŒ¨: {result}")


# --- APScheduler ì„¤ì • ---
if __name__ == "__main__":
    scheduler = BackgroundScheduler(timezone="Asia/Seoul")  # âœ… í•œêµ­ ì‹œê°„ëŒ€ ì§€ì •
    print("ğŸ”„ ìŠ¤ì¼€ì¤„ëŸ¬ ì´ˆê¸°í™”ë¨.")
    # ë§¤ì¼ ì˜¤í›„ 5ì‹œ (17:00)ì— ì‹¤í–‰
    trigger = CronTrigger(hour=10, minute=30)
    scheduler.add_job(job, trigger)

    scheduler.start()
    print("âœ… ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘ë¨. ë§¤ì¼ í•œêµ­ì‹œê°„ ì˜¤í›„ 5ì‹œì— ì‘ì—…ì´ ì‹¤í–‰ë©ë‹ˆë‹¤.")

    try:
        while True:
            time.sleep(1)
    except (KeyboardInterrupt, SystemExit):
        print("ğŸ›‘ ì¢…ë£Œ ì¤‘...")
        scheduler.shutdown()